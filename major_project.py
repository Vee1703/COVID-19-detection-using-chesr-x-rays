# -*- coding: utf-8 -*-
"""MAJOR_PROJECT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l7ZqFZAl7fxxFtvC1nccQcpV7vicLGi-

# LIB
"""

from google.colab import drive
drive.mount('/content/drive')

import warnings
warnings.filterwarnings(action='ignore')

import os
import glob
import shutil
import random
import pandas as pd
from PIL import Image
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt 
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from collections import Counter, defaultdict
import plotly.express as px
from sklearn.decomposition import PCA
import scipy.cluster.hierarchy as shc
from sklearn.cluster import AgglomerativeClustering
from progressbar import ProgressBar
from sklearn.model_selection import train_test_split as tts
import cv2
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import recall_score

"""# Dataset and EDA"""

files = ['COVID','Normal', 'Lung_Opacity','Viral Pneumonia']
path = "/content/drive/MyDrive/COVID"
data_dir = os.path.join(path)

data = []
for id, level in enumerate(files):
    for file in os.listdir(os.path.join(data_dir, level+'/'+'images')):
#         data.append(['{}/{}'.format(level, file), level])
        data.append([level +'/' +'images'+ '/'+file, level])
        

data = pd.DataFrame(data, columns = ['image_file', 'corona_result'])

data['path'] = path + '/' +data['image_file']
data['corona_result'] = data['corona_result'].map({'Normal': 'Normal', 'COVID': 'Covid_positive', 'Lung_Opacity':'Lung_Opacity', 'Viral Pneumonia':'Viral_Pneumonia'})

# data.head() 
data

n_samples = 3

fig, m_axs = plt.subplots(4, n_samples, figsize = (6*3, 3*4))

for n_axs, (type_name, type_rows) in zip(m_axs, data.sort_values(['corona_result']).groupby('corona_result')):
    n_axs[1].set_title(type_name, fontsize = 20)
    for c_ax, (_, c_row) in zip(n_axs, type_rows.sample(3, random_state = 1234).iterrows()):       
        picture = c_row['path']
        image = cv2.imread(picture)
        c_ax.imshow(image)
        c_ax.axis('off')

print('Normal : ', list(data['corona_result']).count('Normal'))
print('Covid : ', list(data['corona_result']).count('Covid_positive'))
print('Opacity : ', list(data['corona_result']).count('Lung_Opacity'))
print('VIral Pneumonia : ', list(data['corona_result']).count('Viral_Pneumonia'))

no_of_samples = 21165
df = pd.DataFrame()
df['corona_result'] = ['Normal', 'Covid_positive', 'Lung_Opacity', 'Viral_Pneumonia']
df['Count'] = [10202,3616,6012,1345]
fig = px.bar(df, x = 'corona_result', y = 'Count', 
             color = "corona_result",  width = 600, 
             color_continuous_scale='BrBg')
fig.show()

from tqdm import tqdm
from PIL import Image, ImageOps

pixel_img = []

for image in tqdm(data['path']):
    img=Image.open(image)
    img=ImageOps.grayscale(img)
    img=img.resize((50,50))
    img=np.asarray(img)
    img=img.reshape((50,50,1))
    img=img.flatten()
    pixel_img.append(img)

pixel_img = np.array(pixel_img)
lbls = data['corona_result'].map({'Normal': 0, 'Covid_positive': 1, 'Lung_Opacity' : 0,
                               'Viral_Pneumonia' : 0})

print(pixel_img.shape)
print(lbls.shape)

plt.figure(figsize=(10,6))
sns.histplot(pixel_img.flatten(), kde=False, bins=50)
plt.title('Distribution of Pixel Values')
plt.xlabel('Pixel Value')
plt.ylabel('Count')
plt.show()

scaler=StandardScaler()
imgs = scaler.fit(pixel_img).transform(pixel_img)

imgs

imgs_train,imgs_test,lbls_train,lbls_test=tts(imgs,lbls,test_size=0.3)

imgs_train.shape, imgs_test.shape

lbls_train=np.array(lbls_train)
lbls_test=np.array(lbls_test)

sns.histplot(lbls)



"""# Anomaly Detection"""

k=len(imgs_train[:,0])**(0.5)
k=int(k)
k

nbrs=NearestNeighbors(n_neighbors=k)
nbrs.fit(imgs_train)

dists,_=nbrs.kneighbors(imgs_train)

plt.plot(dists.mean(axis=1))

d = 55
anoms=np.where(dists.mean(axis=1) > d)[0]
print(len(anoms))

"""anomaly removal"""

anoms

imgs_par=[]
lbls_par=[]
for i in range(len(imgs_train[:,0])):
  if i not in anoms:
    imgs_par.append(imgs_train[i])
    lbls_par.append(lbls_train[i])
imgs_par=np.array(imgs_par)
lbls_par=np.array(lbls_par)

"""# PCA

determining optimal no. of principle components
"""

def pca_variance_plot(x):
  pca=PCA()
  x_pca=pca.fit_transform(x)
  exp_var_pca = pca.explained_variance_ratio_
  plt.plot(range(0,25), [exp_var_pca[i] for i in range(25)], label='Individual explained variance')
  plt.ylabel('Explained variance ratio')
  plt.xlabel('Principal component index')
  plt.legend(loc='best')
  plt.show()

def pca_on_dataset(x,n):
  pca=PCA(n_components=n)
  return(pca.fit(x).transform(x))

pca_variance_plot(imgs_par)

pca_variance_plot(imgs)

n_pc=10
n_pc_par=10

imgs_train_pca=pca_on_dataset(imgs_train,n_pc)

imgs_par_pca=pca_on_dataset(imgs_par,n_pc_par)

imgs_test_pca=pca_on_dataset(imgs_test,n_pc)

"""#Model Selection"""

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import LinearSVC, SVC
from sklearn.neural_network import MLPClassifier
import lightgbm
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier
from xgboost import XGBClassifier
import cv2

def validation_on_models(models, X, y):
    X_train, X_val, y_train, y_val = tts(X, y, train_size=0.85)
    acc_results = []
    recall_results = []
    model_names = []
    for name, model in models.items():
        model.fit(X_train, y_train)
        print(name + " trained")
        result = model.score(X_val, y_val)
        recall = recall_score(y_val, model.predict(X_val))
        acc_results.append(result)
        recall_results.append(recall)
        model_names.append(name)
        print(name + ": accuracy->{:.2f}% recall->{:.2f}%".format(result * 100 , recall*100))
    # plot results
    x = np.arange(len(model_names))
    width = 0.35
    fig, ax = plt.subplots()
    ax.bar(x - width/2, acc_results, width, label='Accuracy')
    ax.bar(x + width/2, recall_results, width, label='Recall')
    ax.set_xticks(x)
    ax.set_xticklabels(model_names, rotation=45, ha='right')
    ax.legend()
    plt.show()

models = {
    "                   Logistic Regression": LogisticRegression(),
    "                   K-Nearest Neighbors": KNeighborsClassifier(),
    "                         Decision Tree": DecisionTreeClassifier(),
    "Support Vector Machine (Linear Kernel)": LinearSVC(),
    "   Support Vector Machine (RBF Kernel)": SVC(),
    "                        Neural Network": MLPClassifier(),
    "                         Random Forest": RandomForestClassifier(),
    "                     Gradient Boosting": GradientBoostingClassifier(),
    "                   XG Boost Classifier": XGBClassifier(),
    "                  Ada Boost Classifier": AdaBoostClassifier(),
    "                   LightGBM Classifier": LGBMClassifier()
}

validation_on_models(models,imgs_train,lbls_train)

validation_on_models(models,imgs_par,lbls_par)

"""#Feature Selection"""

# pip install joblib

# pip install mlxtend --upgrade --no-deps

# import joblib
# from mlxtend.feature_selection import SequentialFeatureSelector as SFS
# from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs

# xgb = XGBClassifier()
# xgb.fit(imgs_par_pca, lbls_par)

# sfs = SFS(xgb, forward=False, floating=True, scoring='recall')
# sfs = sfs.fit(imgs_par, lbls_par)

# print('Accuracy:', sfs.k_score_)
# print('Number of features:', len([list(sfs.k_feature_idx_)]))

from sklearn.feature_selection import SelectFromModel
xgb = XGBClassifier()
xgb.fit(imgs_par_pca, lbls_par)
model = SelectFromModel(xgb, prefit=True)
X_selected = model.transform(imgs_par_pca)
X_selected.shape

X_train,X_val,y_train,y_val=tts(X_selected, lbls_par,train_size=0.85)

xgb_1 = XGBClassifier()
xgb_1.fit(X_train, y_train)
result = xgb_1.score(X_val,y_val)
recall = recall_score(y_val,xgb_1.predict(X_val))

result, recall

"""# Hyperparameter Tuning"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import make_scorer, recall_score

X_train_par,X_val_par,y_train_par,y_val_par=tts(imgs_par,lbls_par,train_size=0.85)

xgb_tuned = XGBClassifier()
param_distributions = {
    'max_depth': [3, 5, 7, 9],
    'subsample': [0.5, 0.7, 1],
    'colsample_bytree': [0.5, 0.7, 1],
    'gamma': [0, 1, 5, 10],
    'reg_lambda': [0, 0.1, 0.5, 1]
}

recall_scorer = make_scorer(recall_score)

random_search = RandomizedSearchCV(
    xgb_tuned,
    param_distributions=param_distributions,
    scoring=recall_scorer,
    verbose=3,
    n_jobs=-1
)

random_search.fit(X_train_par,y_train_par)

best_model = random_search.best_estimator_

y_pred = best_model.predict(X_val_par)
best_model.score(X_val_par,y_val_par)
recall = recall_score(y_val_par, y_pred)

print('Best model recall:', recall)
print(best_model.score(X_val_par,y_val_par))

"""# CNN"""

from tqdm import tqdm
from PIL import Image, ImageOps

pi_cnn = []

for image in tqdm(data['path']):
    img=Image.open(image)
    img=ImageOps.grayscale(img)
    img=img.resize((50,50))
    img=np.asarray(img)
    img=img.reshape((50,50,1))
    pi_cnn.append(img)

pi_cnn = np.array(pi_cnn)
lbls_cnn = data['corona_result'].map({'Normal': 0, 'Covid_positive': 1, 'Lung_Opacity' : 0,
                               'Viral_Pneumonia' : 0})

pi_cnn.shape

(img_train_cnn, img_test_cnn, lbls_train_cnn, lbls_test_cnn) = tts(pi_cnn, lbls_cnn,test_size=0.30,random_state=42)

trainX = np.array(img_train_cnn)
testX = np.array(img_test_cnn)
trainY = np.array(lbls_train_cnn)
testY = np.array(lbls_test_cnn)

print(img_train_cnn.shape, img_test_cnn.shape, lbls_train_cnn.shape, lbls_test_cnn.shape)

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.metrics import classification_report
model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(50,50,1)),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(trainX, trainY, epochs=10, validation_data=(testX, testY))
test_loss, test_acc = model.evaluate(testX, testY)
print('Test accuracy:', test_acc)
predictions = model.predict(testX)
binary_predictions = (predictions > 0.5).astype(int)
print(classification_report(testY, binary_predictions))

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model  accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

from sklearn.model_selection import GridSearchCV
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from tensorflow.keras import models
# Define the hyperparameters to tune
param_grid = {
    'filters': [32, 64],
    'kernel_size': [(3,3), (5,5)],
    'activation': ['relu', 'sigmoid'],
}

# Define a function to build and compile the CNN model with the specified hyperparameters
def build_model(filters, kernel_size, activation, neurons = 32, learning_rate = 0.1):
    model = models.Sequential([
        layers.Conv2D(filters, kernel_size, activation=activation, input_shape=(50,50,1)),
        layers.MaxPooling2D((2,2)),
        layers.Conv2D(filters, kernel_size, activation=activation),
        layers.MaxPooling2D((2,2)),
        layers.Flatten(),
        layers.Dense(neurons, activation=activation),
        layers.Dense(1, activation='sigmoid')
    ])
    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Recall()])
    return model

# Create the GridSearchCV object with the hyperparameters to tune and the build_model function
grid_search = GridSearchCV(estimator=KerasClassifier(build_fn=build_model, epochs=10, validation_data=(testX, testY)), param_grid=param_grid, scoring='recall', verbose=1)

# Train the CNN model with the hyperparameters defined in param_grid and evaluate its performance on the validation set
grid_search.fit(trainX, trainY)

# Print the best hyperparameters and the corresponding recall score
print("Best hyperparameters:", grid_search.best_params_)
print("Best recall score:", grid_search.best_score_)

# Use the best hyperparameters to retrain the model on the entire training set
best_model = build_model(filters=grid_search.best_params_['filters'], kernel_size=grid_search.best_params_['kernel_size'], activation=grid_search.best_params_['activation'], neurons=grid_search.best_params_['neurons'], learning_rate=grid_search.best_params_['learning_rate'])
best_model.fit(trainX, trainY, epochs=10)

# Evaluate the performance of the final model on the test set
test_loss, test_acc, test_recall = best_model.evaluate(testX, testY)
print('Test accuracy:', test_acc)
print('Test recall:', test_recall)